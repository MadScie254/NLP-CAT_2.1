{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7faceca1",
   "metadata": {},
   "source": [
    "# NLP Comparative Analysis Toolkit (NLP-CAT) 2.1: A Comprehensive Study of Text Classification Paradigms\n",
    "\n",
    "**Author:** Daniel Wanjala Machimbo  \n",
    "**Institution:** The Cooperative University of Kenya  \n",
    "**Date:** October 2025  \n",
    "**Python Version:** 3.11.13  \n",
    "\n",
    "---\n",
    "\n",
    "## Reproducibility Badge\n",
    "\n",
    "| Criterion | Status |\n",
    "|-----------|---------|\n",
    "| **Code Available** | ✅ Yes - Complete implementation |\n",
    "| **Data Public** | ✅ Yes - AG News, 20 Newsgroups, IMDb |\n",
    "| **Seeds Fixed** | ✅ Yes - [42, 101, 2023, 7, 999] |\n",
    "| **Environment Specified** | ✅ Yes - requirements.txt provided |\n",
    "| **Statistical Tests** | ✅ Yes - Wilcoxon, Cohen's d, Bootstrap CI |\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Will Produce\n",
    "\n",
    "### Artifacts Generated:\n",
    "- **Models**: `artifacts/classical/`, `artifacts/bilstm/`, `artifacts/bert/`, `artifacts/hybrid/`\n",
    "- **Results**: `results/summary.csv`, `results/statistics.json`\n",
    "- **Applications**: `app_streamlit.py` (React-level dashboard)\n",
    "- **Utilities**: `train.py` (CLI wrapper), `requirements.txt`\n",
    "- **Data**: `data/manifest.json` (dataset checksums)\n",
    "\n",
    "### Commands to Execute:\n",
    "```bash\n",
    "# Run notebook end-to-end (non-interactive)\n",
    "papermill NLP_CAT_comparative_study.ipynb output.ipynb -p run_full true\n",
    "\n",
    "# Launch interactive dashboard\n",
    "streamlit run app_streamlit.py --server.port 8501\n",
    "\n",
    "# Train single model configuration\n",
    "python train.py --dataset ag_news --model bert --n_samples 1000 --seed 42\n",
    "```\n",
    "\n",
    "### Expected Runtime:\n",
    "- **Classical Models**: ~5-10 minutes per dataset\n",
    "- **BiLSTM**: ~15-30 minutes per dataset  \n",
    "- **BERT**: ~45-90 minutes per dataset (GPU), 4-8 hours (CPU)\n",
    "- **Full Experiment Suite**: ~6-12 hours (GPU), ~24-48 hours (CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2c80b",
   "metadata": {},
   "source": [
    "# 1. Abstract\n",
    "\n",
    "This comprehensive study presents a rigorous empirical comparison of four distinct text classification paradigms across three canonical datasets. We systematically evaluate classical machine learning approaches (Multinomial Naïve Bayes and Linear Support Vector Machines with TF-IDF features), recurrent neural networks (Bidirectional LSTM with GloVe embeddings), and modern transformer architectures (BERT-base-uncased) on AG News (4-class news categorization), 20 Newsgroups (20-class discussion forum classification), and IMDb movie reviews (binary sentiment analysis).\n",
    "\n",
    "Our experimental protocol examines model performance across multiple labeled-sample regimes (1K, 5K, 10K, and full datasets) using five independent random seeds to ensure statistical robustness. We employ comprehensive evaluation metrics including accuracy, macro-F1 score, negative log-likelihood, Expected Calibration Error (ECE), per-class performance metrics, inference latency, model size, and computational complexity proxies.\n",
    "\n",
    "**Key Findings** (to be populated after experimentation): Classical methods demonstrate superior computational efficiency and competitive performance on smaller datasets, while transformer models achieve state-of-the-art accuracy at significant computational cost. Our calibration analysis reveals systematic overconfidence in neural models, addressable through temperature scaling. Statistical testing using paired Wilcoxon signed-rank tests and Cohen's d effect sizes provides rigorous significance assessment.\n",
    "\n",
    "This work contributes a reproducible experimental framework with complete statistical analysis, model persistence, and an interactive Streamlit dashboard for real-time model comparison and interpretation. All code, data preprocessing pipelines, and trained models are made available for scientific reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a01ef9",
   "metadata": {},
   "source": [
    "# 2. Problem Statement & Objectives\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Text classification represents a fundamental task in natural language processing with broad applications across information retrieval, content moderation, sentiment analysis, and automated document processing. While the field has witnessed rapid advancement from classical statistical methods to modern transformer architectures, practitioners face critical decisions regarding model selection under varying computational constraints, dataset sizes, and performance requirements.\n",
    "\n",
    "The central research question driving this investigation is: **How do classical machine learning approaches, recurrent neural networks, and transformer models compare across multiple dimensions of performance when evaluated systematically on diverse text classification tasks?**\n",
    "\n",
    "## Research Objectives\n",
    "\n",
    "### Primary Objectives:\n",
    "1. **Comparative Performance Analysis**: Quantify accuracy, calibration, and efficiency trade-offs across four model families\n",
    "2. **Sample Efficiency Assessment**: Characterize learning curves across multiple labeled-sample regimes\n",
    "3. **Statistical Robustness**: Establish significance of performance differences using rigorous statistical testing\n",
    "4. **Practical Deployment Guidance**: Provide actionable insights for model selection in resource-constrained environments\n",
    "\n",
    "## Formal Hypotheses\n",
    "\n",
    "**H1 (Performance Hierarchy)**: Transformer models (BERT) will achieve superior classification accuracy compared to classical and recurrent approaches, with the performance ranking: BERT > BiLSTM > LinearSVM > MultinomialNB.\n",
    "\n",
    "**H2 (Sample Efficiency)**: Classical methods will demonstrate superior performance in low-data regimes (n ≤ 1000), while transformer models will show increasing relative advantage as sample size increases.\n",
    "\n",
    "**H3 (Efficiency-Accuracy Pareto Frontier)**: A clear Pareto frontier will emerge in the accuracy-computational cost space, with classical methods occupying the efficient low-cost region and transformers the high-accuracy high-cost region.\n",
    "\n",
    "**H4 (Calibration Hypothesis)**: Neural models (BiLSTM, BERT) will exhibit systematic overconfidence compared to classical approaches, measurable through Expected Calibration Error (ECE) metrics.\n",
    "\n",
    "## Scientific Significance\n",
    "\n",
    "This study addresses a critical gap in the literature by providing a comprehensive, statistically rigorous comparison across multiple evaluation dimensions. Unlike previous works that focus on accuracy alone, we incorporate calibration assessment, computational efficiency analysis, and robust statistical testing to provide practitioners with actionable insights for model selection in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Reproducibility Configuration\n",
    "# This cell establishes the complete computational environment for our experiments\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# Suppress warnings for cleaner output during experimentation\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for complete reproducibility\n",
    "RANDOM_SEEDS = [42, 101, 2023, 7, 999]\n",
    "DEFAULT_SEED = RANDOM_SEEDS[0]\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(DEFAULT_SEED)\n",
    "np.random.seed(DEFAULT_SEED)\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "os.makedirs('artifacts/classical', exist_ok=True)\n",
    "os.makedirs('artifacts/bilstm', exist_ok=True)\n",
    "os.makedirs('artifacts/bert', exist_ok=True)\n",
    "os.makedirs('artifacts/hybrid', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(\"✓ Directory structure created successfully\")\n",
    "print(f\"✓ Random seeds configured: {RANDOM_SEEDS}\")\n",
    "print(f\"✓ Default seed set to: {DEFAULT_SEED}\")\n",
    "print(f\"✓ Python version: {sys.version}\")\n",
    "print(f\"✓ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241918b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Core Libraries for Data Processing and Machine Learning\n",
    "print(\"Importing core scientific computing libraries...\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_recall_fscore_support, \n",
    "                           confusion_matrix, classification_report, log_loss)\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "import joblib\n",
    "\n",
    "print(\"✓ Sklearn and scipy libraries imported\")\n",
    "\n",
    "# NLP-specific libraries\n",
    "print(\"Importing NLP libraries...\")\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True) \n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"✓ NLTK data downloaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: NLTK download issue: {e}\")\n",
    "\n",
    "print(\"✓ NLP libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Deep Learning Libraries (PyTorch and Transformers)\n",
    "print(\"Importing deep learning libraries...\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "    from torch.optim import Adam, AdamW\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    \n",
    "    # Set PyTorch for reproducibility\n",
    "    torch.manual_seed(DEFAULT_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Check for GPU availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"✓ PyTorch imported successfully - Device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Warning: PyTorch not available - {e}\")\n",
    "    device = 'cpu'\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                            Trainer, TrainingArguments, EarlyStoppingCallback,\n",
    "                            BertTokenizer, BertForSequenceClassification)\n",
    "    \n",
    "    # Set transformers logging level to reduce noise\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    print(\"✓ Transformers library imported successfully\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Transformers not available - {e}\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    from datasets import load_dataset, Dataset as HFDataset\n",
    "    print(\"✓ HuggingFace datasets imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: HuggingFace datasets not available - {e}\")\n",
    "\n",
    "# Import progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"✓ All deep learning libraries configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe792819",
   "metadata": {},
   "source": [
    "# 3. Datasets & Study Area\n",
    "\n",
    "## Dataset Selection Rationale\n",
    "\n",
    "Our experimental design employs three carefully selected datasets that represent distinct text classification challenges across different domains, text lengths, and class distributions:\n",
    "\n",
    "1. **AG News** (Short-form news categorization): 4-class classification with concise, structured text\n",
    "2. **20 Newsgroups** (Medium-form discussion classification): 20-class classification with conversational text\n",
    "3. **IMDb Movie Reviews** (Long-form sentiment analysis): Binary sentiment classification with extended reviews\n",
    "\n",
    "This selection ensures our findings generalize across varying textual characteristics and classification complexity levels.\n",
    "\n",
    "## Ethical Considerations and Data Usage\n",
    "\n",
    "All datasets employed in this study are publicly available, extensively used in academic research, and do not contain personally identifiable information (PII). We acknowledge potential demographic biases present in these datasets and will address fairness considerations in our analysis. Our use complies with respective dataset licenses and academic fair use principles.\n",
    "\n",
    "## Dataset Loading Infrastructure\n",
    "\n",
    "The following implementation provides robust, reproducible dataset loading with comprehensive error handling, caching mechanisms, and metadata tracking. Each dataset is loaded programmatically with fallback options and complete provenance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading Functions with Comprehensive Error Handling\n",
    "@dataclass\n",
    "class DatasetInfo:\n",
    "    \"\"\"Metadata container for dataset information tracking\"\"\"\n",
    "    name: str\n",
    "    source: str\n",
    "    license: str\n",
    "    classes: int\n",
    "    train_size: int\n",
    "    test_size: int\n",
    "    avg_length: float\n",
    "    md5_hash: str\n",
    "    load_time: float\n",
    "\n",
    "def compute_md5_hash(texts: List[str], labels: List[int]) -> str:\n",
    "    \"\"\"Compute MD5 hash of dataset for integrity verification\"\"\"\n",
    "    content = ''.join(texts) + ''.join(map(str, labels))\n",
    "    return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "def load_ag_news_dataset() -> Tuple[List[str], List[int], List[str], List[int], DatasetInfo]:\n",
    "    \"\"\"\n",
    "    Load AG News dataset using HuggingFace datasets with fallback options.\n",
    "    \n",
    "    Source: https://huggingface.co/datasets/ag_news\n",
    "    License: Apache License 2.0\n",
    "    Classes: 4 (World, Sports, Business, Sci/Tech)\n",
    "    \"\"\"\n",
    "    print(\"Loading AG News dataset...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Primary method: HuggingFace datasets\n",
    "        dataset = load_dataset(\"ag_news\", cache_dir=\"data/cache\")\n",
    "        \n",
    "        train_texts = [item['text'] for item in dataset['train']]\n",
    "        train_labels = [item['label'] for item in dataset['train']]\n",
    "        test_texts = [item['text'] for item in dataset['test']]\n",
    "        test_labels = [item['label'] for item in dataset['test']]\n",
    "        \n",
    "        print(f\"✓ AG News loaded via HuggingFace datasets\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"HuggingFace loading failed: {e}\")\n",
    "        print(\"Attempting torchtext fallback...\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback method: torchtext (if available)\n",
    "            import torchtext\n",
    "            from torchtext.datasets import AG_NEWS\n",
    "            \n",
    "            train_iter, test_iter = AG_NEWS(root='data', split=('train', 'test'))\n",
    "            \n",
    "            train_data = list(train_iter)\n",
    "            test_data = list(test_iter)\n",
    "            \n",
    "            train_labels = [int(label) - 1 for label, text in train_data]  # Convert to 0-indexed\n",
    "            train_texts = [text for label, text in train_data]\n",
    "            test_labels = [int(label) - 1 for label, text in test_data]\n",
    "            test_texts = [text for label, text in test_data]\n",
    "            \n",
    "            print(f\"✓ AG News loaded via torchtext fallback\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Torchtext fallback failed: {e2}\")\n",
    "            raise RuntimeError(\"Failed to load AG News dataset with both methods\")\n",
    "    \n",
    "    # Calculate metadata\n",
    "    avg_length = np.mean([len(text.split()) for text in train_texts + test_texts])\n",
    "    md5_hash = compute_md5_hash(train_texts + test_texts, train_labels + test_labels)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    dataset_info = DatasetInfo(\n",
    "        name=\"AG_News\",\n",
    "        source=\"https://huggingface.co/datasets/ag_news\",\n",
    "        license=\"Apache License 2.0\",\n",
    "        classes=4,\n",
    "        train_size=len(train_texts),\n",
    "        test_size=len(test_texts),\n",
    "        avg_length=avg_length,\n",
    "        md5_hash=md5_hash,\n",
    "        load_time=load_time\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ AG News: {dataset_info.train_size} train, {dataset_info.test_size} test samples\")\n",
    "    print(f\"✓ Average text length: {avg_length:.1f} words\")\n",
    "    \n",
    "    return train_texts, train_labels, test_texts, test_labels, dataset_info\n",
    "\n",
    "# Load AG News dataset\n",
    "ag_train_texts, ag_train_labels, ag_test_texts, ag_test_labels, ag_info = load_ag_news_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab00f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_20newsgroups_dataset() -> Tuple[List[str], List[int], List[str], List[int], DatasetInfo]:\n",
    "    \"\"\"\n",
    "    Load 20 Newsgroups dataset using sklearn with header/footer/quote removal.\n",
    "    \n",
    "    Source: sklearn.datasets.fetch_20newsgroups\n",
    "    License: Public Domain\n",
    "    Classes: 20 (various newsgroup categories)\n",
    "    \"\"\"\n",
    "    print(\"Loading 20 Newsgroups dataset...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load with preprocessing to remove headers, footers, and quotes\n",
    "    # This is crucial for fair evaluation as it removes metadata that could be used for cheating\n",
    "    train_data = fetch_20newsgroups(\n",
    "        subset='train', \n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        shuffle=True, \n",
    "        random_state=DEFAULT_SEED,\n",
    "        data_home='data'\n",
    "    )\n",
    "    \n",
    "    test_data = fetch_20newsgroups(\n",
    "        subset='test', \n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        shuffle=True, \n",
    "        random_state=DEFAULT_SEED,\n",
    "        data_home='data'\n",
    "    )\n",
    "    \n",
    "    train_texts = train_data.data\n",
    "    train_labels = train_data.target.tolist()\n",
    "    test_texts = test_data.data\n",
    "    test_labels = test_data.target.tolist()\n",
    "    \n",
    "    # Calculate metadata\n",
    "    avg_length = np.mean([len(text.split()) for text in train_texts + test_texts])\n",
    "    md5_hash = compute_md5_hash(train_texts + test_texts, train_labels + test_labels)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    dataset_info = DatasetInfo(\n",
    "        name=\"20_Newsgroups\",\n",
    "        source=\"sklearn.datasets.fetch_20newsgroups\",\n",
    "        license=\"Public Domain\",\n",
    "        classes=20,\n",
    "        train_size=len(train_texts),\n",
    "        test_size=len(test_texts),\n",
    "        avg_length=avg_length,\n",
    "        md5_hash=md5_hash,\n",
    "        load_time=load_time\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ 20 Newsgroups: {dataset_info.train_size} train, {dataset_info.test_size} test samples\")\n",
    "    print(f\"✓ Average text length: {avg_length:.1f} words\")\n",
    "    print(f\"✓ Target names: {train_data.target_names[:5]}... (showing first 5)\")\n",
    "    \n",
    "    return train_texts, train_labels, test_texts, test_labels, dataset_info\n",
    "\n",
    "# Load 20 Newsgroups dataset\n",
    "ng_train_texts, ng_train_labels, ng_test_texts, ng_test_labels, ng_info = load_20newsgroups_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_dataset() -> Tuple[List[str], List[int], List[str], List[int], DatasetInfo]:\n",
    "    \"\"\"\n",
    "    Load IMDb movie reviews dataset using HuggingFace datasets.\n",
    "    \n",
    "    Source: https://huggingface.co/datasets/imdb\n",
    "    License: Apache License 2.0\n",
    "    Classes: 2 (positive, negative sentiment)\n",
    "    \"\"\"\n",
    "    print(\"Loading IMDb dataset...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Primary method: HuggingFace datasets\n",
    "        dataset = load_dataset(\"imdb\", cache_dir=\"data/cache\")\n",
    "        \n",
    "        train_texts = [item['text'] for item in dataset['train']]\n",
    "        train_labels = [item['label'] for item in dataset['train']]\n",
    "        test_texts = [item['text'] for item in dataset['test']]\n",
    "        test_labels = [item['label'] for item in dataset['test']]\n",
    "        \n",
    "        print(f\"✓ IMDb loaded via HuggingFace datasets\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"HuggingFace loading failed: {e}\")\n",
    "        print(\"Attempting tensorflow_datasets fallback...\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback method: tensorflow_datasets (if available)\n",
    "            import tensorflow_datasets as tfds\n",
    "            \n",
    "            ds_train = tfds.load('imdb_reviews', split='train', as_supervised=True, \n",
    "                               data_dir='data/tfds_cache')\n",
    "            ds_test = tfds.load('imdb_reviews', split='test', as_supervised=True,\n",
    "                              data_dir='data/tfds_cache')\n",
    "            \n",
    "            train_texts = []\n",
    "            train_labels = []\n",
    "            for text, label in ds_train:\n",
    "                train_texts.append(text.numpy().decode('utf-8'))\n",
    "                train_labels.append(int(label.numpy()))\n",
    "                \n",
    "            test_texts = []\n",
    "            test_labels = []\n",
    "            for text, label in ds_test:\n",
    "                test_texts.append(text.numpy().decode('utf-8'))\n",
    "                test_labels.append(int(label.numpy()))\n",
    "            \n",
    "            print(f\"✓ IMDb loaded via tensorflow_datasets fallback\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"TensorFlow datasets fallback failed: {e2}\")\n",
    "            raise RuntimeError(\"Failed to load IMDb dataset with both methods\")\n",
    "    \n",
    "    # Calculate metadata\n",
    "    avg_length = np.mean([len(text.split()) for text in train_texts + test_texts])\n",
    "    md5_hash = compute_md5_hash(train_texts + test_texts, train_labels + test_labels)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    dataset_info = DatasetInfo(\n",
    "        name=\"IMDb\",\n",
    "        source=\"https://huggingface.co/datasets/imdb\",\n",
    "        license=\"Apache License 2.0\", \n",
    "        classes=2,\n",
    "        train_size=len(train_texts),\n",
    "        test_size=len(test_texts),\n",
    "        avg_length=avg_length,\n",
    "        md5_hash=md5_hash,\n",
    "        load_time=load_time\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ IMDb: {dataset_info.train_size} train, {dataset_info.test_size} test samples\")\n",
    "    print(f\"✓ Average text length: {avg_length:.1f} words\")\n",
    "    \n",
    "    return train_texts, train_labels, test_texts, test_labels, dataset_info\n",
    "\n",
    "# Load IMDb dataset\n",
    "imdb_train_texts, imdb_train_labels, imdb_test_texts, imdb_test_labels, imdb_info = load_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c35de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Metadata Tracking and Manifest Generation\n",
    "def save_dataset_manifest(datasets_info: List[DatasetInfo]) -> None:\n",
    "    \"\"\"Save dataset metadata to JSON manifest for reproducibility tracking\"\"\"\n",
    "    manifest = {\n",
    "        'generated_at': datetime.now().isoformat(),\n",
    "        'python_version': sys.version,\n",
    "        'random_seed': DEFAULT_SEED,\n",
    "        'datasets': {info.name: asdict(info) for info in datasets_info}\n",
    "    }\n",
    "    \n",
    "    with open('data/manifest.json', 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Dataset manifest saved to data/manifest.json\")\n",
    "\n",
    "# Collect all dataset information\n",
    "all_datasets_info = [ag_info, ng_info, imdb_info]\n",
    "\n",
    "# Save manifest\n",
    "save_dataset_manifest(all_datasets_info)\n",
    "\n",
    "# Display dataset summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df = pd.DataFrame([asdict(info) for info in all_datasets_info])\n",
    "summary_df = summary_df[['name', 'classes', 'train_size', 'test_size', 'avg_length', 'load_time']]\n",
    "summary_df['avg_length'] = summary_df['avg_length'].round(1)\n",
    "summary_df['load_time'] = summary_df['load_time'].round(2)\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Exploratory Data Analysis (EDA)\n",
    "def perform_dataset_eda(texts: List[str], labels: List[int], dataset_name: str, \n",
    "                       label_names: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform comprehensive exploratory data analysis on a text dataset.\n",
    "    \n",
    "    Returns statistical summaries and generates publication-quality visualizations.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing {dataset_name} dataset...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    n_samples = len(texts)\n",
    "    n_classes = len(set(labels))\n",
    "    \n",
    "    # Text length analysis\n",
    "    text_lengths = [len(text.split()) for text in texts]\n",
    "    length_stats = {\n",
    "        'mean': np.mean(text_lengths),\n",
    "        'std': np.std(text_lengths),\n",
    "        'min': np.min(text_lengths),\n",
    "        'max': np.max(text_lengths),\n",
    "        'median': np.median(text_lengths),\n",
    "        'q25': np.percentile(text_lengths, 25),\n",
    "        'q75': np.percentile(text_lengths, 75)\n",
    "    }\n",
    "    \n",
    "    # Class distribution analysis\n",
    "    class_counts = pd.Series(labels).value_counts().sort_index()\n",
    "    class_distribution = {\n",
    "        'counts': class_counts.to_dict(),\n",
    "        'proportions': (class_counts / class_counts.sum()).to_dict(),\n",
    "        'imbalance_ratio': class_counts.max() / class_counts.min()\n",
    "    }\n",
    "    \n",
    "    # Character-level statistics\n",
    "    char_lengths = [len(text) for text in texts]\n",
    "    char_stats = {\n",
    "        'mean_chars': np.mean(char_lengths),\n",
    "        'std_chars': np.std(char_lengths)\n",
    "    }\n",
    "    \n",
    "    # Vocabulary analysis (approximate)\n",
    "    all_words = []\n",
    "    for text in texts[:1000]:  # Sample for efficiency\n",
    "        all_words.extend(text.lower().split())\n",
    "    \n",
    "    vocab_stats = {\n",
    "        'unique_words_sample': len(set(all_words)),\n",
    "        'total_words_sample': len(all_words),\n",
    "        'avg_word_length': np.mean([len(word) for word in all_words])\n",
    "    }\n",
    "    \n",
    "    eda_results = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'n_samples': n_samples,\n",
    "        'n_classes': n_classes,\n",
    "        'length_stats': length_stats,\n",
    "        'char_stats': char_stats,\n",
    "        'class_distribution': class_distribution,\n",
    "        'vocab_stats': vocab_stats\n",
    "    }\n",
    "    \n",
    "    # Create publication-quality visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'{dataset_name} Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Text length distribution\n",
    "    axes[0, 0].hist(text_lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(length_stats['mean'], color='red', linestyle='--', \n",
    "                       label=f'Mean: {length_stats[\"mean\"]:.1f}')\n",
    "    axes[0, 0].axvline(length_stats['median'], color='green', linestyle='--', \n",
    "                       label=f'Median: {length_stats[\"median\"]:.1f}')\n",
    "    axes[0, 0].set_xlabel('Text Length (words)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Text Length Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Class distribution\n",
    "    class_labels = label_names if label_names else [f'Class {i}' for i in range(n_classes)]\n",
    "    if len(class_labels) <= 10:  # Full labels for manageable number of classes\n",
    "        axes[0, 1].bar(range(len(class_counts)), class_counts.values, \n",
    "                       color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].set_xticks(range(len(class_counts)))\n",
    "        axes[0, 1].set_xticklabels([class_labels[i] for i in class_counts.index], \n",
    "                                   rotation=45, ha='right')\n",
    "    else:  # Simplified for many classes\n",
    "        axes[0, 1].bar(range(len(class_counts)), class_counts.values, \n",
    "                       color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].set_xlabel('Class Index')\n",
    "    \n",
    "    axes[0, 1].set_ylabel('Sample Count')\n",
    "    axes[0, 1].set_title(f'Class Distribution (Imbalance Ratio: {class_distribution[\"imbalance_ratio\"]:.2f})')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Length vs Class boxplot (for reasonable number of classes)\n",
    "    if n_classes <= 10:\n",
    "        length_by_class = [[] for _ in range(n_classes)]\n",
    "        for text, label in zip(texts, labels):\n",
    "            length_by_class[label].append(len(text.split()))\n",
    "        \n",
    "        axes[1, 0].boxplot(length_by_class, labels=[f'C{i}' for i in range(n_classes)])\n",
    "        axes[1, 0].set_xlabel('Class')\n",
    "        axes[1, 0].set_ylabel('Text Length (words)')\n",
    "        axes[1, 0].set_title('Text Length Distribution by Class')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Alternative visualization for many classes\n",
    "        axes[1, 0].scatter(labels[:1000], [len(texts[i].split()) for i in range(1000)], \n",
    "                          alpha=0.5, s=1)\n",
    "        axes[1, 0].set_xlabel('Class Index')\n",
    "        axes[1, 0].set_ylabel('Text Length (words)')\n",
    "        axes[1, 0].set_title('Text Length vs Class (Sample)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Summary statistics table\n",
    "    axes[1, 1].axis('off')\n",
    "    stats_text = f'''\n",
    "    Dataset Statistics:\n",
    "    \n",
    "    Samples: {n_samples:,}\n",
    "    Classes: {n_classes}\n",
    "    \n",
    "    Text Length (words):\n",
    "      Mean: {length_stats[\"mean\"]:.1f} ± {length_stats[\"std\"]:.1f}\n",
    "      Median: {length_stats[\"median\"]:.1f}\n",
    "      Range: [{length_stats[\"min\"]}, {length_stats[\"max\"]}]\n",
    "      \n",
    "    Characters per text:\n",
    "      Mean: {char_stats[\"mean_chars\"]:.0f} ± {char_stats[\"std_chars\"]:.0f}\n",
    "      \n",
    "    Class Balance:\n",
    "      Most frequent: {class_counts.max():,} samples\n",
    "      Least frequent: {class_counts.min():,} samples\n",
    "      Imbalance ratio: {class_distribution[\"imbalance_ratio\"]:.2f}\n",
    "    '''\n",
    "    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                     fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/{dataset_name.lower()}_eda.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return eda_results\n",
    "\n",
    "# Perform EDA for all datasets\n",
    "print(\"Conducting comprehensive exploratory data analysis...\")\n",
    "\n",
    "# AG News EDA\n",
    "ag_class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "ag_eda = perform_dataset_eda(ag_train_texts, ag_train_labels, 'AG_News', ag_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9718641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue EDA for remaining datasets and display sample texts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 20 Newsgroups EDA  \n",
    "ng_class_names = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', \n",
    "                  'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    "                  'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    "                  'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med',\n",
    "                  'sci.space', 'soc.religion.christian', 'talk.politics.guns',\n",
    "                  'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
    "ng_eda = perform_dataset_eda(ng_train_texts, ng_train_labels, '20_Newsgroups', ng_class_names)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# IMDb EDA\n",
    "imdb_class_names = ['Negative', 'Positive'] \n",
    "imdb_eda = perform_dataset_eda(imdb_train_texts, imdb_train_labels, 'IMDb', imdb_class_names)\n",
    "\n",
    "# Display sample texts from each dataset for qualitative understanding\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE TEXTS FROM EACH DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def display_sample_texts(texts: List[str], labels: List[int], dataset_name: str, \n",
    "                        class_names: List[str], n_samples: int = 2) -> None:\n",
    "    \"\"\"Display sample texts from each class for qualitative analysis\"\"\"\n",
    "    print(f\"\\n{dataset_name} Sample Texts:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    unique_labels = sorted(set(labels))\n",
    "    for label in unique_labels[:min(len(unique_labels), 4)]:  # Show up to 4 classes\n",
    "        label_indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        sample_indices = np.random.choice(label_indices, min(n_samples, len(label_indices)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        print(f\"\\nClass: {class_names[label] if label < len(class_names) else f'Class_{label}'}\")\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            text_preview = texts[idx][:200] + \"...\" if len(texts[idx]) > 200 else texts[idx]\n",
    "            print(f\"  Sample {i+1}: {text_preview}\")\n",
    "            print()\n",
    "\n",
    "# Set random seed for consistent sampling\n",
    "np.random.seed(DEFAULT_SEED)\n",
    "\n",
    "display_sample_texts(ag_train_texts, ag_train_labels, \"AG News\", ag_class_names)\n",
    "display_sample_texts(ng_train_texts, ng_train_labels, \"20 Newsgroups\", ng_class_names)  \n",
    "display_sample_texts(imdb_train_texts, imdb_train_labels, \"IMDb\", imdb_class_names)\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c4994",
   "metadata": {},
   "source": [
    "# 4. Preprocessing Pipeline\n",
    "\n",
    "## Text Preprocessing Philosophy and Implementation\n",
    "\n",
    "Text preprocessing represents a critical yet often underappreciated component of NLP pipeline design. Our approach implements a flexible, modular preprocessing framework that enables systematic ablation studies while maintaining reproducibility across different model architectures.\n",
    "\n",
    "### Preprocessing Considerations:\n",
    "\n",
    "1. **Normalization**: Converting text to consistent case and removing extraneous characters\n",
    "2. **Tokenization**: Word-level vs. subword tokenization strategies  \n",
    "3. **Stop Word Removal**: Impact on different classification paradigms\n",
    "4. **Lemmatization**: Computational cost vs. potential benefit analysis\n",
    "5. **Feature Engineering**: N-gram extraction and TF-IDF parameterization\n",
    "\n",
    "### Design Principles:\n",
    "\n",
    "- **Modularity**: Each preprocessing step can be toggled independently\n",
    "- **Consistency**: Identical preprocessing for fair model comparison\n",
    "- **Efficiency**: Optimized implementations with caching for large datasets\n",
    "- **Reproducibility**: Deterministic operations with fixed parameters\n",
    "\n",
    "The following implementation provides comprehensive preprocessing utilities with extensive parameter control, enabling both classical feature extraction and modern tokenizer compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Text Preprocessing Pipeline\n",
    "import re\n",
    "import string\n",
    "from typing import Callable\n",
    "\n",
    "# Initialize preprocessing components\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(\"✓ NLTK preprocessing components initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: NLTK initialization issue: {e}\")\n",
    "    stop_words = set()\n",
    "    lemmatizer = None\n",
    "\n",
    "@dataclass  \n",
    "class PreprocessingConfig:\n",
    "    \"\"\"Configuration class for text preprocessing parameters\"\"\"\n",
    "    lowercase: bool = True\n",
    "    remove_punctuation: bool = True\n",
    "    remove_digits: bool = False\n",
    "    remove_stopwords: bool = True\n",
    "    lemmatize: bool = False\n",
    "    min_token_length: int = 2\n",
    "    max_token_length: int = 50\n",
    "\n",
    "def clean_text(text: str, config: PreprocessingConfig = PreprocessingConfig()) -> str:\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function with configurable options.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        config: PreprocessingConfig object with cleaning parameters\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Basic cleaning - remove excessive whitespace and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    # Convert to lowercase if specified\n",
    "    if config.lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove URLs, email addresses, and mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove digits if specified\n",
    "    if config.remove_digits:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation if specified (preserve word boundaries)\n",
    "    if config.remove_punctuation:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Normalize whitespace again after punctuation removal\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text: str, config: PreprocessingConfig = PreprocessingConfig()) -> List[str]:\n",
    "    \"\"\"\n",
    "    Advanced tokenization with filtering and optional lemmatization.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        config: PreprocessingConfig object with tokenization parameters\n",
    "        \n",
    "    Returns:\n",
    "        List of processed tokens\n",
    "    \"\"\"\n",
    "    # Clean text first\n",
    "    text = clean_text(text, config)\n",
    "    \n",
    "    # Tokenize using NLTK word_tokenize (handles contractions better than split())\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except:\n",
    "        # Fallback to simple split if NLTK fails\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # Filter tokens by length\n",
    "    tokens = [token for token in tokens \n",
    "              if config.min_token_length <= len(token) <= config.max_token_length]\n",
    "    \n",
    "    # Remove stopwords if specified\n",
    "    if config.remove_stopwords and stop_words:\n",
    "        tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Apply lemmatization if specified and available\n",
    "    if config.lemmatize and lemmatizer:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_tfidf_vectorizer(ngram_range: Tuple[int, int] = (1, 1),\n",
    "                           max_features: int = 10000,\n",
    "                           use_idf: bool = True,\n",
    "                           preprocessing_config: PreprocessingConfig = PreprocessingConfig()) -> TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    Create configured TF-IDF vectorizer with custom preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        ngram_range: Tuple of (min_n, max_n) for n-gram extraction\n",
    "        max_features: Maximum number of features to extract\n",
    "        use_idf: Whether to use IDF weighting\n",
    "        preprocessing_config: Text preprocessing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Configured TfidfVectorizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    def custom_preprocessor(text: str) -> str:\n",
    "        \"\"\"Custom preprocessor function for TfidfVectorizer\"\"\"\n",
    "        return clean_text(text, preprocessing_config)\n",
    "    \n",
    "    def custom_tokenizer(text: str) -> List[str]:\n",
    "        \"\"\"Custom tokenizer function for TfidfVectorizer\"\"\"\n",
    "        return tokenize_text(text, preprocessing_config)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        preprocessor=custom_preprocessor,\n",
    "        tokenizer=custom_tokenizer,\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=max_features,\n",
    "        use_idf=use_idf,\n",
    "        lowercase=False,  # Already handled in custom preprocessor\n",
    "        stop_words=None,  # Already handled in custom tokenizer\n",
    "        dtype=np.float32  # Use float32 for memory efficiency\n",
    "    )\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "# Test preprocessing pipeline with examples\n",
    "def test_preprocessing_pipeline():\n",
    "    \"\"\"Test and demonstrate preprocessing pipeline functionality\"\"\"\n",
    "    \n",
    "    test_texts = [\n",
    "        \"Hello World! This is a TEST with numbers 123 and punctuation...\",\n",
    "        \"Check out this URL: https://example.com and email test@email.com\",\n",
    "        \"Multiple    spaces   and\\t\\ttabs should be normalized!!!\",\n",
    "        \"Contractions like don't, won't, and I'm should be handled properly.\"\n",
    "    ]\n",
    "    \n",
    "    # Test different configurations\n",
    "    configs = {\n",
    "        'minimal': PreprocessingConfig(lowercase=True, remove_punctuation=False, \n",
    "                                     remove_stopwords=False, lemmatize=False),\n",
    "        'standard': PreprocessingConfig(lowercase=True, remove_punctuation=True, \n",
    "                                      remove_stopwords=True, lemmatize=False),\n",
    "        'aggressive': PreprocessingConfig(lowercase=True, remove_punctuation=True, \n",
    "                                        remove_stopwords=True, remove_digits=True, \n",
    "                                        lemmatize=True)\n",
    "    }\n",
    "    \n",
    "    print(\"PREPROCESSING PIPELINE TESTING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\n{config_name.upper()} Configuration:\")\n",
    "        print(f\"Config: {config}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for i, text in enumerate(test_texts[:2]):  # Test first 2 for brevity\n",
    "            cleaned = clean_text(text, config)\n",
    "            tokens = tokenize_text(text, config)\n",
    "            \n",
    "            print(f\"Original {i+1}: {text}\")\n",
    "            print(f\"Cleaned {i+1}:  {cleaned}\")\n",
    "            print(f\"Tokens {i+1}:   {tokens}\")\n",
    "            print()\n",
    "\n",
    "# Run preprocessing tests\n",
    "test_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93384d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer-Compatible Preprocessing Functions\n",
    "def prepare_transformer_inputs(texts: List[str], labels: List[int], \n",
    "                              tokenizer_name: str = 'bert-base-uncased',\n",
    "                              max_length: int = 128, \n",
    "                              padding: str = 'max_length',\n",
    "                              truncation: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prepare input data for transformer models using HuggingFace tokenizers.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input texts\n",
    "        labels: List of corresponding labels\n",
    "        tokenizer_name: Name of the tokenizer to use\n",
    "        max_length: Maximum sequence length\n",
    "        padding: Padding strategy\n",
    "        truncation: Whether to truncate long sequences\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing tokenized inputs and labels\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, \n",
    "                                                cache_dir='data/cache/transformers')\n",
    "        \n",
    "        # Tokenize texts\n",
    "        print(f\"Tokenizing {len(texts)} texts with {tokenizer_name}...\")\n",
    "        \n",
    "        # Batch tokenization for efficiency\n",
    "        encoding = tokenizer(\n",
    "            texts,\n",
    "            truncation=truncation,\n",
    "            padding=padding,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'labels': labels_tensor,\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Tokenization complete. Shape: {encoding['input_ids'].shape}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in transformer preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Preprocessing Ablation Study\n",
    "def run_preprocessing_ablation(texts: List[str], labels: List[int], \n",
    "                             dataset_name: str, n_samples: int = 1000) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run ablation study on preprocessing choices to quantify their impact.\n",
    "    \n",
    "    This provides evidence-based guidance for preprocessing decisions.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRUNNING PREPROCESSING ABLATION FOR {dataset_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample data for faster ablation\n",
    "    if len(texts) > n_samples:\n",
    "        indices = np.random.choice(len(texts), n_samples, replace=False)\n",
    "        sample_texts = [texts[i] for i in indices]\n",
    "        sample_labels = [labels[i] for i in indices]\n",
    "    else:\n",
    "        sample_texts, sample_labels = texts, labels\n",
    "    \n",
    "    # Split for quick evaluation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        sample_texts, sample_labels, test_size=0.3, random_state=DEFAULT_SEED, \n",
    "        stratify=sample_labels\n",
    "    )\n",
    "    \n",
    "    # Test different preprocessing configurations\n",
    "    ablation_configs = {\n",
    "        'baseline': PreprocessingConfig(lowercase=False, remove_punctuation=False, \n",
    "                                      remove_stopwords=False, lemmatize=False),\n",
    "        'lowercase': PreprocessingConfig(lowercase=True, remove_punctuation=False, \n",
    "                                       remove_stopwords=False, lemmatize=False),\n",
    "        'no_punct': PreprocessingConfig(lowercase=True, remove_punctuation=True, \n",
    "                                      remove_stopwords=False, lemmatize=False),\n",
    "        'no_stopwords': PreprocessingConfig(lowercase=True, remove_punctuation=True, \n",
    "                                          remove_stopwords=True, lemmatize=False),\n",
    "        'full': PreprocessingConfig(lowercase=True, remove_punctuation=True, \n",
    "                                  remove_stopwords=True, lemmatize=True)\n",
    "    }\n",
    "    \n",
    "    ablation_results = {}\n",
    "    \n",
    "    for config_name, config in ablation_configs.items():\n",
    "        try:\n",
    "            # Create vectorizer with current config\n",
    "            vectorizer = create_tfidf_vectorizer(\n",
    "                ngram_range=(1, 1), \n",
    "                max_features=5000, \n",
    "                preprocessing_config=config\n",
    "            )\n",
    "            \n",
    "            # Fit and transform\n",
    "            start_time = time.perf_counter()\n",
    "            X_train_vec = vectorizer.fit_transform(X_train)\n",
    "            X_val_vec = vectorizer.transform(X_val)\n",
    "            vectorize_time = time.perf_counter() - start_time\n",
    "            \n",
    "            # Quick classification test\n",
    "            clf = MultinomialNB()\n",
    "            clf.fit(X_train_vec, y_train)\n",
    "            val_acc = clf.score(X_val_vec, y_val)\n",
    "            \n",
    "            # Store results\n",
    "            ablation_results[config_name] = {\n",
    "                'accuracy': val_acc,\n",
    "                'vocab_size': len(vectorizer.vocabulary_),\n",
    "                'vectorize_time': vectorize_time,\n",
    "                'feature_density': X_train_vec.nnz / X_train_vec.shape[0]  # Avg features per sample\n",
    "            }\n",
    "            \n",
    "            print(f\"{config_name:12} | Acc: {val_acc:.3f} | Vocab: {len(vectorizer.vocabulary_):5d} | \"\n",
    "                  f\"Time: {vectorize_time:.2f}s | Density: {X_train_vec.nnz / X_train_vec.shape[0]:.1f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{config_name:12} | ERROR: {e}\")\n",
    "            ablation_results[config_name] = {'error': str(e)}\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find best configuration\n",
    "    valid_results = {k: v for k, v in ablation_results.items() if 'error' not in v}\n",
    "    if valid_results:\n",
    "        best_config = max(valid_results.keys(), key=lambda k: valid_results[k]['accuracy'])\n",
    "        print(f\"Best preprocessing configuration: {best_config} \"\n",
    "              f\"(Accuracy: {valid_results[best_config]['accuracy']:.3f})\")\n",
    "    \n",
    "    return ablation_results\n",
    "\n",
    "# Run ablation studies for all datasets\n",
    "print(\"Conducting preprocessing ablation studies...\")\n",
    "\n",
    "# Set random seed for consistent ablation\n",
    "np.random.seed(DEFAULT_SEED)\n",
    "random.seed(DEFAULT_SEED)\n",
    "\n",
    "# Run ablations (using smaller sample sizes for efficiency during development)\n",
    "ag_ablation = run_preprocessing_ablation(ag_train_texts, ag_train_labels, \"AG_News\", n_samples=500)\n",
    "ng_ablation = run_preprocessing_ablation(ng_train_texts, ng_train_labels, \"20_Newsgroups\", n_samples=500) \n",
    "imdb_ablation = run_preprocessing_ablation(imdb_train_texts, imdb_train_labels, \"IMDb\", n_samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16953b02",
   "metadata": {},
   "source": [
    "# 5. Baseline Classical Models (Detailed Implementation + Narrative)\n",
    "\n",
    "## Classical Machine Learning: The Foundation of Text Classification\n",
    "\n",
    "Before the deep learning revolution transformed NLP, classical machine learning approaches dominated text classification tasks. These methods, particularly Multinomial Naïve Bayes and Support Vector Machines with TF-IDF features, established the fundamental principles that continue to influence modern approaches.\n",
    "\n",
    "### Theoretical Foundations:\n",
    "\n",
    "**Multinomial Naïve Bayes (MNB)**: Based on Bayes' theorem with the \"naïve\" assumption of feature independence. Despite this strong assumption being violated in natural language, MNB often performs surprisingly well due to its robustness and the prevalence of discriminative features in text.\n",
    "\n",
    "**Linear Support Vector Machines (LinearSVM)**: Implements the principle of structural risk minimization, finding the optimal hyperplane that maximizes the margin between classes. The linear kernel is particularly well-suited for high-dimensional sparse text features.\n",
    "\n",
    "**TF-IDF Features**: Term Frequency-Inverse Document Frequency creates a vector space representation where each dimension represents a term's importance, balancing local term frequency with global discriminative power.\n",
    "\n",
    "### Implementation Strategy:\n",
    "\n",
    "Our implementation employs scikit-learn's robust pipeline infrastructure, enabling systematic hyperparameter optimization while maintaining clean separation of concerns between preprocessing, feature extraction, and classification.\n",
    "\n",
    "### Performance Considerations:\n",
    "\n",
    "Classical methods excel in computational efficiency, interpretability, and performance on smaller datasets. They serve as essential baselines and often remain competitive with more complex approaches, particularly when computational resources are constrained."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
